# 8.1. 前提

要部署超融合Proxmox +  Ceph集群，你必须使用至少3个相同的服务器进行设置。可以查看[Ceph 网站](https://docs.ceph.com/en/nautilus/start/hardware-recommendations/)的建议

## CPU

高CPU 频率可以减少延迟，应该是首选。根据经验，你应该为每个Ceph服务分配（预留）一个CPU内核（或线程），以便为稳定和持久的 Ceph 性能提供足够的资源。

## 内存

在超融合设置中，尤其需要仔细监控内存消耗。除了预测的虚拟机和容器的内存使用量之外，您还必须考虑有足够的内存可供 Ceph 使用，以提供出色而稳定的性能。

根据经验，对于大约1 TiB 的数据， OSD 将使用 1 GiB 的内存。特别是在恢复、重新平衡或回填期间。

守护进程本身将使用额外的内存。默认情况下，守护程序的 Bluestore 后端需要3-5 GiB 的内存（可调整）。相比之下，传统的 Filestore 后端使用 OS 页面缓存，内存消耗通常与 OSD 守护进程的 PG 有关。


## 网络

建议为Ceph准备专用的10Gb或者更高性能的网络。如果没有10Gb交换机设备，也可以使用网状网络[ Ceph网状网络配置参见https://pve.proxmox.com/wiki/Full_Mesh_Network_for_Ceph_Server]。

高负载网络通信，特别是虚拟机恢复时的流量，将影响运行在同一网络上的服务，很有可能造成Proxmox VE集群崩溃。

建议认真估算网络带宽需求。单块硬盘可能不能压满1Gb链路，但多块硬盘组成的OSD就可以。主流NVME SSD完全可以压满10Gbps带宽。采用更高带宽性能的网络，可以确保网络任何时候都不会成为性能瓶颈。为此，25Gb，40Gb，100Gb的网络都值得考虑。
存储盘

在规划Ceph集群时，需要重点考虑恢复时间因素。对于小规模集群，恢复时间可能会非常长。推荐在小规模集群中使用固态SSD盘代替HDD硬盘，以缩短恢复时间，降低恢复期间发生二次故障的风险。

通常情况下，SSD的IOPs比传统磁盘高的多，但价格也更贵，可以参考4.2.9节组建不同类型的存储池，以提高恢复性能。也可以参考4.2.7节内容，使用高速存储盘作为DB/WAL设备，加速OSDs。如果同时为多个OSDs配置了高速存储盘，需要考虑平衡OSD和WAL/DB（卷）盘的配比，以避免高速存储盘成为相关OSDs的性能瓶颈。

除了选择合适存储盘类型，还可以选择为单一节点配置偶数个对称存储盘，以提高Ceph性能。例如，单一节点使用4块500GB存储盘时的性能就比混合使用1块1TB盘和3块250GB盘要好。
此外，还需要妥善平衡OSD数量和单一OSD容量。大容量OSD可以增加存储密度，但也意味着在OSD故障时，Ceph需要恢复更多数据。

## 不要使用硬RAID
Ceph直接处理数据对象冗余和多重并发磁盘（OSDs）写操作，因此使用硬RAID控制器并不能提高性能和可用性。相反，Ceph需要直接控制磁盘硬件设备。硬件RAID控制器并非为Ceph所设计，其写操作管理和缓存算法可能干扰Ceph对磁盘的正常操作，从而把事情复杂化，并导致性能降低。

*警告* 不要使用硬件RAID控制器，可改用主机HBA卡。

以上是关于硬件选型的一个粗略建议。具体还要结合需求特点，并对部署进行测试，以及对Ceph健康状况和性能进行持续观测，才能判定是否满足需要。

